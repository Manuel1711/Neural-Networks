{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeurNet(tf.Module):\n",
    "    def __init__(self, sigma_w, sigma_b, Nboot, Mu_, STD_, activation, depth, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        #self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.activation = activation\n",
    "        self.lsizes = np.concatenate(([input_size], np.repeat(hidden_size, depth-1), [output_size]))\n",
    "        self.sigma_w = sigma_w\n",
    "        self.sigma_b = sigma_b\n",
    "        self.depth = depth\n",
    "        self.mu = Mu_\n",
    "        self.std_dev = STD_\n",
    "        self.Nboot = Nboot\n",
    "        #self.Ws = np.empty((Nboot, depth), dtype=object)  \n",
    "        #self.Bs = np.empty((Nboot, depth), dtype=object)\n",
    "        self.Ws = self.initialize_weights()\n",
    "        self.Bs = self.initialize_biases()\n",
    "        '''\n",
    "        for i in range(Nboot):\n",
    "            for l in range(depth):\n",
    "                bname = 'b{}'.format(l)\n",
    "                Wname = 'W{}'.format(l)\n",
    "                self.Ws[i, l] = tf.Variable(tf.random.normal([self.lsizes[l], self.lsizes[l+1]], mean=Mu_, stddev=STD_), name=Wname)\n",
    "                self.Bs[i, l] = tf.Variable(tf.random.normal([self.lsizes[l+1]], mean=Mu_, stddev=STD_), name=bname)\n",
    "        '''\n",
    "    def initialize_weights(self):\n",
    "        Ws = np.empty((self.Nboot, self.depth), dtype=object)\n",
    "        for i in range(self.Nboot):\n",
    "            for l in range(self.depth):\n",
    "                Wname = 'W{}'.format(l)\n",
    "                Ws[i, l] = tf.Variable(tf.random.normal([self.lsizes[l], self.lsizes[l+1]], mean=self.mu, stddev=self.std_dev), name=Wname)\n",
    "        return Ws\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        Bs = np.empty((self.Nboot, self.depth), dtype=object)\n",
    "        for i in range(self.Nboot):\n",
    "            for l in range(self.depth):\n",
    "                bname = 'b{}'.format(l)\n",
    "                Bs[i, l] = tf.Variable(tf.random.normal([self.lsizes[l+1]], mean=self.mu, stddev=self.std_dev), name=bname)\n",
    "        return Bs\n",
    "    \n",
    "    def forward(self, X, i):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch([param for param in [self.Ws[i, j] for j in range(self.depth)] + [self.Bs[i, j] for j in range(self.depth)]])\n",
    "\n",
    "            A = X\n",
    "            for j in range(self.depth):\n",
    "                Z = tf.add(tf.matmul(A, self.sigma_w/self.lsizes[j]**0.5*self.Ws[i, j]), self.sigma_b*self.Bs[i, j])\n",
    "                A = self.activation(Z)\n",
    "\n",
    "            Z_last = Z\n",
    "\n",
    "            #gradients = tape.gradient(Z_last, [self.Ws[i, j] for j in range(self.depth)] + [self.Bs[i, j] for j in range(self.depth)])\n",
    "        return Z_last#, gradients\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, X_train, Y_train, eta, i):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Z = self.forward(X_train, i)\n",
    "            loss = tf.losses.mean_squared_error(Z, Y_train)  # Define loss func using Z and x_train\n",
    "            optimizer = tf.keras.optimizers.Adam()\n",
    "            #variables = [param for param in [self.Ws[i, j] for j in range(self.depth)] + [self.Bs[i, j] for j in range(self.depth)]]\n",
    "            variables = [param for param in [self.Ws[i, j] for j in range(self.depth)] + [self.Bs[i, j] for j in range(self.depth)]]\n",
    "            optimizer.build(variables)\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return Z, loss\n",
    "    \n",
    "    def training_measure(self, X_train, Y_train, eta, num_epochs, X_out, Nout, i):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.obser_sum = np.ndarray(shape=(num_epochs, 3, Nout, self.Nboot), dtype=float, buffer=np.zeros((num_epochs, 3, Nout, self.Nboot)))\n",
    "        for epoch in range(num_epochs):\n",
    "            Z, loss = self.train_step(X_train, Y_train, eta, i)\n",
    "            loss_value = tf.reduce_mean(loss)\n",
    "\n",
    "            Z = self.forward(X_out, i)\n",
    "            Z2 = tf.square(Z)\n",
    "            #print(Z.numpy())\n",
    "            self.obser_sum[epoch, 0, :, i] = np.reshape(Z.numpy(), (Nout,) )\n",
    "            self.obser_sum[epoch, 1, :, i] = np.reshape(Z2.numpy(), (Nout,) )\n",
    "            #obser_sum[epoch, 2, :, i] = np.reshape(loss.numpy(), (Nout,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45922444",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nin = 2\n",
    "Nout = 100\n",
    "xx1 = tf.constant(-1., shape=(Nin, 1, 1), dtype=tf.float32)\n",
    "xx2 = tf.constant(1., shape=(Nin, 1, 1), dtype=tf.float32)\n",
    "\n",
    "def interp(_alpha):\n",
    "    condition1 = tf.logical_and(_alpha <= 1, _alpha >= 0)\n",
    "    condition2 = _alpha > 1\n",
    "    condition3 = _alpha < 0\n",
    "    \n",
    "    result = tf.where(condition1, _alpha * xx1 + (1 - _alpha) * xx2, tf.where(condition2, xx1, xx2))\n",
    "    return result\n",
    "\n",
    "alpha_train = tf.random.uniform(shape=(Nin,1,1), minval=-2, maxval=3, dtype=tf.float32)\n",
    "x_train = interp(alpha_train)\n",
    "\n",
    "alpha_out = tf.random.uniform(shape=(Nout,1,1), minval=-2, maxval=3, dtype=tf.float32)\n",
    "\n",
    "#print(x_train)\n",
    "#sys.exit()\n",
    "\n",
    "sigma_w = np.sqrt(1.5)\n",
    "sigma_b = 0.\n",
    "Mu = 0.\n",
    "STD = 1.\n",
    "Nboot = 10\n",
    "depth = 3\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "eta = 0.5\n",
    "\n",
    "# Numb of train epochs\n",
    "num_epochs = 500\n",
    "\n",
    "rete = NeurNet(sigma_w, sigma_b, Nboot, Mu, STD, tf.nn.tanh, depth, input_size, hidden_size, output_size)\n",
    "\n",
    "for i in range(Nboot):\n",
    "    rete.training_measure(alpha_train, x_train, eta, num_epochs, alpha_out, Nout, i)\n",
    "'''\n",
    "lsizes = np.concatenate(([input_size], np.repeat(hidden_size, depth-1), [output_size]))\n",
    "#print(lsizes)\n",
    "\n",
    "#loss_fun = tf.losses.mean_squared_error()\n",
    "\n",
    "# obser_sum = [mean output net, dev_std output net, loss]\n",
    "obser_sum = np.ndarray(shape=(num_epochs, 3, Nout, Nboot), dtype=float, buffer=np.zeros((num_epochs, 3, Nout, Nboot)))\n",
    "\n",
    "for i in range(Nboot):\n",
    "\n",
    "    for l in range(depth):\n",
    "        Ws[i, l], Bs[i, l] = initialization(lsizes[l], lsizes[l+1], Mu, STD / lsizes[l]**0.5, l)\n",
    "        #print(l, Ws[i,l].shape)\n",
    "\n",
    "    # Optimizer definition \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # Train function\n",
    "    @tf.function\n",
    "    def train_step(X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Z, gradients = forward(X, i, tf.nn.tanh, depth)\n",
    "            loss = tf.losses.mean_squared_error(Z, x_train)  # Define loss func using Z and x_train\n",
    "\n",
    "        variables = [param for param in [Ws[i, j] for j in range(depth)] + [Bs[i, j] for j in range(depth)]]\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return Z, loss\n",
    "\n",
    "    # Train cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        Z, loss = train_step(alpha_train, x_train)\n",
    "        loss_value = tf.reduce_mean(loss)\n",
    "        \n",
    "        Z, _ = forward(alpha_out, i, tf.nn.tanh, depth)\n",
    "        Z2 = tf.square(Z)\n",
    "        #print(Z.numpy())\n",
    "        obser_sum[epoch, 0, :, i] = np.reshape(Z.numpy(), (Nout,) )\n",
    "        obser_sum[epoch, 1, :, i] = np.reshape(Z2.numpy(), (Nout,) )\n",
    "        #obser_sum[epoch, 2, :, i] = np.reshape(loss.numpy(), (Nout,) )\n",
    "'''\n",
    "alpha_np = alpha_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1 = tf.constant(-1., shape=(Nout, 1, 1), dtype=tf.float32)\n",
    "xx2 = tf.constant(1., shape=(Nout, 1, 1), dtype=tf.float32)\n",
    "\n",
    "alpha_np = alpha_out.numpy()\n",
    "alpha_np = np.reshape(alpha_out, (Nout,))\n",
    "\n",
    "x_np = interp(alpha_out)\n",
    "x_np = np.reshape(x_np.numpy(), (Nout,))\n",
    "\n",
    "epoch_fix = 499\n",
    "\n",
    "y = np.array([0.]*len(alpha_np))\n",
    "erry = np.array([0.]*len(alpha_np))\n",
    "\n",
    "for i in range(len(alpha_np)):\n",
    "    y[i] = mean(rete.obser_sum[epoch_fix, 0, i, :])\n",
    "    erry[i] = np.sqrt(mean(rete.obser_sum[epoch_fix, 1, i, :]) - y[i]**2.)#std_dev(Nboot*obser_sum[epoch_fix, 0, i, :])\n",
    "\n",
    "print(erry)\n",
    "\n",
    "#print(mean(obser_sum[epoch_fix, 0, 0, :]), std_dev(obser_sum[epoch_fix, 0, 0, :])*Nboot**0.5, (mean(obser_sum[epoch_fix, 1, 0, :]) - mean(obser_sum[epoch_fix, 0, 0, :])**2.)**0.5)\n",
    "\n",
    "#err = np.sqrt(obser_sum[epoch_fix, 1, :] - obser_sum[epoch_fix, 0, :]**2.)\n",
    "#print(alpha_np[0:5], x_np[0:5], obser_sum[400, 0, 0:5], obser_sum[400, 2, 0:5])\n",
    "plt.errorbar(alpha_np, y, yerr=2.*erry, fmt = '.', capsize=1)\n",
    "plt.errorbar(alpha_np, x_np, fmt = '.', capsize=1)\n",
    "plt.errorbar(np.reshape(alpha_train.numpy(), (Nin, )), np.reshape(x_train.numpy(), (Nin,)), fmt = 'o', capsize=30)\n",
    "plt.show()\n",
    "\n",
    "# PUT THE BIAS AT ZERO AND CHANGE THE DEV_STD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
